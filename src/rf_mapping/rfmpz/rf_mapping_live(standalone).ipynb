{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF mapping amination: a stand-alone notebook\n",
    "\n",
    "Tony Fu, July 2022\n",
    "\n",
    "Note: I simplified many functions here. Please see my github (https://github.com/tonyfu97/borderownership) for full implementations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights, VGG16_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the unit (i.e., convolutional kernel) of interest:\n",
    "\n",
    "**NOTE**: conv_i = 0 means conv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "model_name = 'alexnet'\n",
    "# model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "# model_name = 'vgg16'\n",
    "\n",
    "conv_i = 0\n",
    "unit_i = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the mapping parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bar drawing functions (written by Dr. Bair, slightly modified by me)\n",
    "\n",
    "Note1: njit is a just-in-time compiler that is recommended for functions\n",
    "       that are called very often. The stimfr_bar() function is sped up\n",
    "       by about 180x thanks to it. See Numba documentation for details.\n",
    "      \n",
    "Note2: The y-axis points DOWN.\n",
    "\"\"\"\n",
    "\n",
    "@njit\n",
    "def clip(val, vmin, vmax):\n",
    "    \"\"\"Limits value to be vmin <= val <= vmax\"\"\"\n",
    "    if vmin > vmax:\n",
    "        raise Exception(\"vmin should be smaller than vmax.\")\n",
    "    val = min(val, vmax)\n",
    "    val = max(val, vmin)\n",
    "    return val\n",
    "\n",
    "@njit\n",
    "def rotate(dx, dy, theta_deg):\n",
    "    \"\"\"\n",
    "    Applies the rotation matrix:\n",
    "        [dx, dy] * [[a, b], [c, d]]  = [dx_r, dy_r]\n",
    "    \n",
    "    To undo rotation, apply again with negative theta_deg.\n",
    "    Modified from Dr. Wyeth Bair's d06_mrf.py\n",
    "    \"\"\"\n",
    "    thetar = theta_deg * math.pi / 180.0\n",
    "    rot_a = math.cos(thetar); rot_b = math.sin(thetar)\n",
    "    rot_c = math.sin(thetar); rot_d = -math.cos(thetar)\n",
    "    # Usually, the negative sign appears in rot_c instead of rot_d, but the\n",
    "    # y-axis points downward in our case.\n",
    "    dx_r = rot_a*dx + rot_c*dy\n",
    "    dy_r = rot_b*dx + rot_d*dy\n",
    "    return dx_r, dy_r\n",
    "\n",
    "@njit\n",
    "def stimfr_bar(xn, yn, x0, y0, theta, blen, bwid, aa, fgval, bgval):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    xn    - (int) horizontal width of returned array\\n\n",
    "    yn    - (int) vertical height of returned array\\n\n",
    "    x0    - (float) horizontal offset of center (pix)\\n\n",
    "    y0    - (float) vertical offset of center (pix)\\n\n",
    "    theta - (float) orientation (pix)\\n\n",
    "    blen  - (float) length of bar (pix)\\n\n",
    "    bwid  - (float) width of bar (pix)\\n\n",
    "    aa    - (float) length scale for anti-aliasing (pix)\\n\n",
    "    fgval - (float) bar luminance [0..1]\\n\n",
    "    bgval - (float) background luminance [0..1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Return a numpy array that has a bar on a background.\n",
    "    \"\"\"\n",
    "    s = np.full((yn,xn), bgval, dtype='float32')  # Fill w/ BG value\n",
    "    dval = fgval - bgval  # Luminance difference\n",
    "  \n",
    "    # Maximum extent of unrotated bar corners in zero-centered frame:\n",
    "    dx = bwid/2.0\n",
    "    dy = blen/2.0\n",
    "\n",
    "    # Rotate top-left corner from (-dx, dy) to (dx1, dy1)\n",
    "    dx1, dy1 = rotate(-dx, dy, theta)\n",
    "    \n",
    "    # Rotate top-right corner from (dx, dy) to (dx2, dy2)\n",
    "    dx2, dy2 = rotate(dx, dy, theta)\n",
    "\n",
    "    # Maximum extent of rotated bar corners in zero-centered frame:\n",
    "    maxx = aa + max(abs(dx1), abs(dx2))\n",
    "    maxy = aa + max(abs(dy1), abs(dy2))\n",
    "    \n",
    "    # Center of stimulus field\n",
    "    xc = (xn-1.0)/2.0\n",
    "    yc = (yn-1.0)/2.0\n",
    "\n",
    "    # Define the 4 corners a box that contains the rotated bar.\n",
    "    bar_left_i   = round(xc + x0 - maxx)\n",
    "    bar_right_i  = round(xc + x0 + maxx) + 1\n",
    "    bar_top_i    = round(yc + y0 - maxy)\n",
    "    bar_bottom_i = round(yc + y0 + maxy) + 1\n",
    "\n",
    "    bar_left_i   = clip(bar_left_i  , 0, xn)\n",
    "    bar_right_i  = clip(bar_right_i , 0, xn)\n",
    "    bar_top_i    = clip(bar_top_i   , 0, yn)\n",
    "    bar_bottom_i = clip(bar_bottom_i, 0, yn)\n",
    "\n",
    "    for i in range(bar_left_i, bar_right_i):  # for i in range(0,xn):\n",
    "        xx = i - xc - x0  # relative to bar center\n",
    "        for j in range (bar_top_i, bar_bottom_i):  # for j in range (0,yn):\n",
    "            yy = j - yc - y0  # relative to bar center\n",
    "            x, y = rotate(xx, yy, -theta)  # rotate back\n",
    "\n",
    "            # Compute distance from bar edge, 'db'\n",
    "            if x > 0.0:\n",
    "                dbx = bwid/2 - x  # +/- indicates inside/outside\n",
    "            else:\n",
    "                dbx = x + bwid/2\n",
    "\n",
    "            if y > 0.0:\n",
    "                dby = blen/2 - y  # +/- indicates inside/outside\n",
    "            else:\n",
    "                dby = y + blen/2\n",
    "\n",
    "            if dbx < 0.0:  # x outside\n",
    "                if dby < 0.0:\n",
    "                    db = -math.sqrt(dbx*dbx + dby*dby)  # Both outside\n",
    "                else:\n",
    "                    db = dbx\n",
    "            else:  # x inside\n",
    "                if dby < 0.0:  # y outside\n",
    "                    db = dby\n",
    "                else:  # Both inside - take the smallest distance\n",
    "                    if dby < dbx:\n",
    "                        db = dby\n",
    "                    else:\n",
    "                        db = dbx\n",
    "\n",
    "            if aa > 0.0:\n",
    "                if db > aa:\n",
    "                    f = 1.0  # This point is inside the bar\n",
    "                elif db < -aa:\n",
    "                    f = 0.0  # This point is outside the bar\n",
    "                else:  # Use sinusoidal sigmoid\n",
    "                    f = 0.5 + 0.5*math.sin(db/aa * 0.25*math.pi)\n",
    "            else:\n",
    "                if db >= 0.0:\n",
    "                    f = 1.0  # inside\n",
    "                else:\n",
    "                    f = 0.0  # outside\n",
    "\n",
    "            s[j, i] += f * dval  # add a fraction 'f' of the 'dval'\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since each set of bar stimuli are designed for a single layer, it will be a\n",
    "waste of time to present them to later layers that we are not going to analyze.\n",
    "Therefore, here is a function that truncates the model and returns the output\n",
    "tensor of the specified layer.\n",
    "\"\"\"\n",
    "\n",
    "def truncated_model(x, model, layer_index):\n",
    "    \"\"\"\n",
    "    Returns the output of the specified layer without forward passing to\n",
    "    the subsequent layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.tensor\n",
    "        The input. Should have dimension (1, 3, 2xx, 2xx).\n",
    "    model : torchvision.model.Module\n",
    "        The neural network (or the layer if in a recursive case).\n",
    "    layer_index : int\n",
    "        The index of the layer, the output of which will be returned. The\n",
    "        indexing excludes container layers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y : torch.tensor\n",
    "        The output of layer with the layer_index.\n",
    "    layer_index : int\n",
    "        Used for recursive cases. Should be ignored.\n",
    "    \"\"\"\n",
    "    # If the layer is not a container, forward pass.\n",
    "    if (len(list(model.children())) == 0):\n",
    "        return model(x), layer_index - 1\n",
    "    else:  # Recurse otherwise.\n",
    "        for sublayer in model.children():\n",
    "            x, layer_index = truncated_model(x, sublayer, layer_index)\n",
    "            if layer_index < 0:  # Stop at the specified layer.\n",
    "                return x, layer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 51, 99, 131, 163]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "According to the internet, a \"hook\" is *a place and usually an interface\n",
    "provided in packaged code that allows a programmer to insert customized\n",
    "programming*. Pytorch allows users to register hook functions in the network.\n",
    "A 'forward hook' is a function that will be automatically evoked whenever the\n",
    "network does a forward pass. Similarly, a 'backward hook' is evoked whenever\n",
    "the network does a backward pass (i.e., backprop). A forward hook must be\n",
    "formatted as follows:\n",
    "\n",
    "            hook_function_name(module, ten_in, ten_out)\n",
    "\n",
    "and a backward hook must be formatted as follows:\n",
    "\n",
    "            hook_function_name(module, grad_in, grad_out)\n",
    "\n",
    "You may overwrite the tensor/gradient output by returning something else. Next,\n",
    "to register a forward hook to a layer object, do this:\n",
    "\n",
    "            layer.register_forward_hook(hook_function_name)\n",
    "\n",
    "To register a backward hook, do this:\n",
    "\n",
    "            layer.register_backward_hook(hook_function_name)\n",
    "\n",
    "Since a hook function cannot return anything except the tensor/gradient output\n",
    "of a layer, it is not very useful on its own. Therefore, it is usually written\n",
    "as a class method, so that it can write to or read from the class attributes\n",
    "when it is evoked. The following block of code uses a hook function to\n",
    "find the maximum sizes of the receptive fields (RF) of all convolutional layers\n",
    "of a given network.\n",
    "\"\"\"\n",
    "\n",
    "class HookFunctionBase:\n",
    "    \"\"\"\n",
    "    A base class that register a hook function to all specified layer types\n",
    "    (excluding all container types) in a given model. The child class must\n",
    "    implement hook_function(). The child class must also call\n",
    "    self.register_forward_hook_to_layers() by itself.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, layer_types):\n",
    "        \"\"\"\n",
    "        Constructs a HookFunctionBase object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torchvision.models\n",
    "            The neural network.\n",
    "        layer_types : tuple of torch.nn.Modules\n",
    "            A tuple of the layer types you would like to register the forward\n",
    "            hook to. For example, layer_types = (nn.Conv2d, nn.ReLU) means\n",
    "            that all the Conv2d and ReLU layers will be registered with the\n",
    "            forward hook.\n",
    "        \"\"\"\n",
    "        self.model = copy.deepcopy(model)  # DEEPCOPY IS A MUST\n",
    "        self.layer_types = layer_types\n",
    "\n",
    "    def hook_function(self, module, ten_in, ten_out):\n",
    "        raise NotImplementedError(\"Child class of HookFunctionBase must \"\n",
    "                                  \"implement hookfunction(self, module, ten_in, ten_out)\")\n",
    "\n",
    "    def register_forward_hook_to_layers(self, layer):\n",
    "        # If \"model\" is a leave node and matches the layer_type, register hook.\n",
    "        if (len(list(layer.children())) == 0):\n",
    "            if (isinstance(layer, self.layer_types)):\n",
    "                layer.register_forward_hook(self.hook_function)\n",
    "\n",
    "        # Otherwise (i.e.,the layer is a container type layer), recurse.\n",
    "        else:\n",
    "            for sublayer in layer.children():\n",
    "                self.register_forward_hook_to_layers(sublayer)\n",
    "\n",
    "\n",
    "class RfSize(HookFunctionBase):\n",
    "    def __init__(self, model, image_size):\n",
    "        \"\"\"\n",
    "        Constructs a RfSize object that calculates the sizes of the receptive\n",
    "        fields (RFs), assuming all RFs are square. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : torchvision.Module\n",
    "            The neural network.\n",
    "        image_size : int\n",
    "            The size of the input image. This should not really matter unless\n",
    "            the image size is smaller than the RF size.\n",
    "        \"\"\"\n",
    "        super().__init__(model, layer_types=(torch.nn.Module))\n",
    "        self.image_size = image_size\n",
    "        self.layer_indices = []\n",
    "        self.rf_sizes = []\n",
    "        self.rf_size = None\n",
    "        self.layer_counter = 0\n",
    "        self.strides = [1]\n",
    "        self.kernel_sizes = [1]\n",
    "        self.need_convsersion = (nn.Conv2d, nn.AvgPool2d, nn.MaxPool2d)\n",
    "        \n",
    "        self.register_forward_hook_to_layers(self.model)\n",
    "    \n",
    "    def _clip(self, val, vmin, vmax):\n",
    "        \"\"\"Limits value to be vmin <= val <= vmax\"\"\"\n",
    "        if vmin > vmax:\n",
    "            raise Exception(\"vmin should be smaller than vmax.\")\n",
    "        val = min(val, vmax)\n",
    "        val = max(val, vmin)\n",
    "        return val\n",
    "    \n",
    "    def _calculate_rf(self, stride, kernel_size):\n",
    "        \"\"\"Calculates the RF size of this layer.\"\"\"\n",
    "        if isinstance(stride, tuple):\n",
    "            stride = stride[0]\n",
    "        if isinstance(kernel_size, tuple):\n",
    "            kernel_size = kernel_size[0]\n",
    "\n",
    "        tmp_rf_size = kernel_size\n",
    "        for s, ks in zip(self.strides[::-1], self.kernel_sizes[::-1]):\n",
    "            \"\"\"\n",
    "            The function:\n",
    "\n",
    "                tmp_rf_size = ks_{n-1} + (ks_{n} - 1) * s_{n-1}\n",
    "\n",
    "            calculates how many units of the previous layer (n-1) are covered\n",
    "            by {tmp_rf_size} (right-hand side) units of the current layer (n).\n",
    "            When applied iteratively from using the kernel_size (ks) and stride\n",
    "            (s) from the current layer to the pixel layer, we can calcualte the\n",
    "            how many pixels are ultimately projected to one unit in the current\n",
    "            layer.\n",
    "\n",
    "            For example:\n",
    "            conv1 in    * * * * * * * * * * * * * * * * *\n",
    "            --------    - - - - -\n",
    "            ks1 = 5         | - - - - -\n",
    "            s1  = 3         |     | - - - - -\n",
    "            rf1 = 5         |     |     | - - - - -\n",
    "                            |     |     |     | - - - - -\n",
    "                            |     |     |     |     |\n",
    "                            V     V     V     V     V\n",
    "            conv1 out       *     *     *     *     *\n",
    "            ---------       -     -     -\n",
    "            ks2 = 3               +     -     -\n",
    "            s2  = 1               |     -     -     -\n",
    "            rf2 = 11              |     |     |\n",
    "                                  V     V     V\n",
    "                                  *     *     *\n",
    "                                  -     -     -\n",
    "                                        |\n",
    "                                        V\n",
    "            conv2 out                   *\n",
    "            ---------\n",
    "            ks3 = 3\n",
    "            rf3 = 15\n",
    "            \"\"\"\n",
    "            tmp_rf_size = ks + (tmp_rf_size - 1) * s\n",
    "            \n",
    "        self.strides.append(stride)\n",
    "        self.kernel_sizes.append(kernel_size)\n",
    "        return tmp_rf_size\n",
    "\n",
    "    def hook_function(self, module, ten_in, ten_out):\n",
    "        if isinstance(module, self.need_convsersion):\n",
    "            self.rf_size = self._calculate_rf(module.stride, module.kernel_size)\n",
    "            self.rf_size = self._clip(self.rf_size, 0, self.image_size)\n",
    "        \n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Record layer index and RF size of if module is a conv layers.\n",
    "            self.layer_indices.append(self.layer_counter)\n",
    "            self.rf_sizes.append(self.rf_size)\n",
    "        \n",
    "        self.layer_counter += 1 \n",
    "    \n",
    "    def calculate(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        layer_indicies : [int, ...]\n",
    "            The indices of all convolutional layers.\n",
    "        rf_sizes : [int, ...]\n",
    "            The RF sizes of all convolutional layers.\n",
    "        \"\"\"\n",
    "        dummy_input = torch.ones((1, 3, self.image_size, self.image_size))\n",
    "        _ = self.model(dummy_input)\n",
    "        return self.layer_indices, self.rf_sizes\n",
    "    \n",
    "\n",
    "model = models.alexnet()\n",
    "rf_size_object = RfSize(model, 227)\n",
    "layer_indices, rf_sizes = rf_size_object.calculate()\n",
    "print(rf_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is how we define the 'center' point of a space.\"\"\"\n",
    "\n",
    "def calculate_center(output_size):\n",
    "    \"\"\"\n",
    "    center = (output_size - 1)//2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_size : int or (int, int)\n",
    "        The size of the output maps in (height, width) format.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The index (int) or indices (int, int) of the spatial center.\n",
    "    \"\"\"\n",
    "    if isinstance(output_size, (tuple, list, np.ndarray)):\n",
    "        if len(output_size) != 2:\n",
    "            raise ValueError(\"output_size should have a length of 2.\")\n",
    "        c1 = calculate_center(output_size[0])\n",
    "        c2 = calculate_center(output_size[1])\n",
    "        return c1, c2\n",
    "    else:\n",
    "        return (output_size - 1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The RF of the early layers are small, so it will be a waste of computational\n",
    "resources to present the full image of size 227x277. The following function\n",
    "calculates the image sizes (of all conv layers) that are just big enough to\n",
    "fit the RFs of the center units, while centering the bar such that the padding\n",
    "in all directions are the same.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def xn_to_center_rf(model):\n",
    "    \"\"\"\n",
    "    Return the input image size xn = yn just big enough to center the RF of\n",
    "    the center units (of all Conv2d layer) in the pixel space. Need this\n",
    "    function because we don't want to use the full image size (227, 227).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torchvision.models\n",
    "        The neural network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xn_list : [int, ...]\n",
    "        A list of xn (which is also yn since we assume RF to be square). \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rf_size_object = RfSize(model, 227)\n",
    "    layer_indices, rf_sizes = rf_size_object.calculate()\n",
    "    xn_list = []\n",
    "    \n",
    "    for layer_index, rf_size in zip(layer_indices, rf_sizes):\n",
    "        # Set before and after to different values first\n",
    "        center_response_before = -2\n",
    "        center_response_after = -1\n",
    "        rf_size = rf_size[0]\n",
    "        xn = int(rf_size * 1.1)  # add a 10% padding.\n",
    "\n",
    "        # If response before and after perturbation are identical, the unit\n",
    "        # RF is centered.\n",
    "        while(center_response_before != center_response_after):\n",
    "            xn += 1\n",
    "            dummy_input = torch.rand((1, 3, xn, xn))\n",
    "            y, _ = truncated_model(dummy_input, model, layer_index)\n",
    "            yc, xc = calculate_center(y.shape[-2:])\n",
    "            center_response_before = y[0, 0, yc, xc].item()\n",
    "            \n",
    "            # Skip this loop if the paddings on two sides aren't equal.\n",
    "            if ((xn - rf_size)%2 != 0):\n",
    "                continue\n",
    "\n",
    "            padding = (xn - rf_size) // 2\n",
    "            \n",
    "            # Add perturbation to the surrounding padding.\n",
    "            dummy_input[:, :,  :padding,  :padding] = 10000\n",
    "            dummy_input[:, :, -padding:, -padding:] = 10000\n",
    "            y, _ = truncated_model(dummy_input, model, layer_index)\n",
    "            center_response_after = y[0, 0, yc, xc].item()\n",
    "\n",
    "        xn_list.append(xn)\n",
    "    return xn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################.#######################################\n",
    "#                                                                             #\n",
    "#                             STIM_DAPP_BAR_XYO_BW                            #\n",
    "#                                                                             #\n",
    "#  Create dictionary entries for bar stimuli varying in these parameters:     #\n",
    "#    x location                                                               #\n",
    "#    y location                                                               #\n",
    "#    orientation                                                              #\n",
    "#    luminance contrast polarity                                              #\n",
    "#                                                                             #\n",
    "#  The input parameters specify the range of x-values to use, and these       #\n",
    "#  are replicated for the y-range as well.                                    #\n",
    "#  'dori' to use for varying orientation.                                     #\n",
    "#                                                                             #\n",
    "#  The other bar parameters are held fixed:  length, width, anti-aliasing.    #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "def stim_dapp_bar_xyo_bw(splist,xn,xlist,orilist,blen,bwid,aa):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    splist  - stimulus parameter list - APPEND TO THIS LIST\\n\n",
    "    xn      - horizontal and vertical image size\\n\n",
    "    xlist   - list of x-coordinates (pix)\\n\n",
    "    orilist - list of orientation values (degr)\\n\n",
    "    blen    - Length of bar (pix)\\n\n",
    "    bwid    - Width of bar (pix)\\n\n",
    "    aa      - Anti-aliasing space constant (pix)\\n\n",
    "    \"\"\"\n",
    "    yn = xn        # Assuming image is square\n",
    "    ylist = xlist  # Use same coordinates for y grid locations\n",
    "    \n",
    "    fgval =  1.0  # Foreground luminance\n",
    "    bgval = -1.0  # Background luminance\n",
    "    \n",
    "    # nstim = len(xlist) * len(ylist) * len(orilist) * 2\n",
    "    # print(\"  Creating \", nstim, \" stimulus dictionary entries.\")\n",
    "    \n",
    "    for i in xlist:\n",
    "        for j in ylist:\n",
    "            for o in orilist:\n",
    "                tp = {\"xn\":xn, \"yn\":yn, \"x0\":i, \"y0\":j, \"theta\":o, \"len\":blen,\n",
    "                      \"wid\":bwid, \"aa\":aa, \"fgval\":fgval, \"bgval\":bgval}\n",
    "                splist.append(tp)\n",
    "                \n",
    "                # Now swap 'bgval' and 'fgval' to make opposite contrast\n",
    "                tp = {\"xn\":xn, \"yn\":yn, \"x0\":i, \"y0\":j, \"theta\":o, \"len\":blen,\n",
    "                      \"wid\":bwid, \"aa\":aa, \"fgval\":bgval, \"bgval\":fgval}\n",
    "                splist.append(tp)\n",
    "\n",
    "\n",
    "#######################################.#######################################\n",
    "#                                                                             #\n",
    "#                             STIMSET_GRIDX_BARMAP                            #\n",
    "#                                                                             #\n",
    "#  Given a bar length and maximum RF size (both in pixels), return a list     #\n",
    "#  of the x-coordinates of the grid points relative to the center of the      #\n",
    "#  image field.                                                               #\n",
    "#                                                                             #\n",
    "#  I believe the following are true:                                          #\n",
    "#  (1) The center coordinate \"0.0\" will always be included                    #\n",
    "#  (2) There will be an odd number of coordinates                             #\n",
    "#  (3) The extreme coordinates will never be more then half of a bar length   #\n",
    "#      outside of the maximum RF ('max_rf')                                   #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "def stimset_gridx_barmap(max_rf,blen):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_rf - maximum RF size (pix)\\n\n",
    "    blen   - bar length (pix)\\n\n",
    "    \"\"\"\n",
    "    dx = blen / 2.0                       # Grid spacing is 1/2 of bar length\n",
    "    xmax = round((max_rf/dx) / 2.0) * dx  # Max offset of grid point from center\n",
    "    xlist = np.arange(-xmax,xmax+1,dx)\n",
    "\n",
    "    return xlist\n",
    "\n",
    "\n",
    "#######################################.#######################################\n",
    "#                                                                             #\n",
    "#                            STIMSET_DICT_RFMP_4A                             #\n",
    "#                                                                             #\n",
    "#  Return the stimulus parameter dictionary with the appropriate entries      #\n",
    "#  for the entire stimulus set for RF mapping paradigm \"4a\".                  #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "def stimset_dict_rfmp_4a(xn,max_rf):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    xn     - stimulus image size (pix)\\n\n",
    "    max_rf - maximum RF size (pix)\\n\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    splist - List of dictionary entries, one per stimulus image.\n",
    "    \"\"\"\n",
    "    splist = []\n",
    "\n",
    "    #  There are 4 bar lengths\n",
    "    barlen = np.array([48/64 * max_rf,    #  Array of bar lengths\n",
    "                       24/64 * max_rf,\n",
    "                       12/64 * max_rf,\n",
    "                        6/64 * max_rf])\n",
    "\n",
    "    #  There are 3 aspect ratios\n",
    "    arat = np.array([1/2, 1/5, 1/10])   # Array of aspect ratios\n",
    "\n",
    "    #  There are 16 orientations, even spaced around 360 deg starting at 0 deg\n",
    "    orilist = np.arange(0.0, 180.0, 22.5)\n",
    "\n",
    "    #  This constant sets how much blurring occurs at the edges of the bars\n",
    "    aa =  0.5      # Antialias distance (pix)\n",
    "\n",
    "    for bl in barlen:\n",
    "        xlist = stimset_gridx_barmap(max_rf,bl)\n",
    "        for ar in arat:\n",
    "            stim_dapp_bar_xyo_bw(splist,xn,xlist,orilist,bl,ar*bl,aa)\n",
    "\n",
    "    # print(\"  Length of stimulus parameter list:\",len(splist))\n",
    "\n",
    "    return splist\n",
    "\n",
    "\n",
    "#######################################.#######################################\n",
    "#                                                                             #\n",
    "#                                BARMAP_RUN_01b                               #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "def barmap_run_01b(splist, model, layer_idx, num_units, batch_size=100,\n",
    "                  _debug=False):\n",
    "    \"\"\"\n",
    "    Presents bars and returns the center responses in array of dimension:\n",
    "    [num_stim, num_units].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    splist     - bar stimulus parameter list.\\n\n",
    "    model      - neural network.\\n\n",
    "    layer_idx  - index of the layer of interest.\\n\n",
    "    num_units  - number of units/channels.\\n\n",
    "    batch_size - how many bars to present at once.\\n\n",
    "    _debug     - if true, reduce the number of bars and plot them.\\n\n",
    "    \"\"\"\n",
    "    bar_i = 0\n",
    "    num_stim = len(splist)\n",
    "    xn = splist[0]['xn']\n",
    "    yn = splist[0]['yn']\n",
    "    center_responses = np.zeros((num_stim, num_units))\n",
    "\n",
    "    while (bar_i < num_stim):\n",
    "        if _debug and bar_i > 500:\n",
    "            break\n",
    "        real_batch_size = min(batch_size, num_stim-bar_i)\n",
    "        bar_batch = np.zeros((real_batch_size, 3, yn, xn))\n",
    "\n",
    "        # Create a batch of bars.\n",
    "        for i in range(real_batch_size):\n",
    "            params = splist[bar_i + i]\n",
    "            new_bar = stimfr_bar(params['xn'], params['yn'],\n",
    "                                 params['x0'], params['y0'],\n",
    "                                params['theta'], params['len'], params['wid'], \n",
    "                                params['aa'], params['fgval'], params['bgval'])\n",
    "            # Replicate new bar to all color channel.\n",
    "            bar_batch[i, 0] = new_bar\n",
    "            bar_batch[i, 1] = new_bar\n",
    "            bar_batch[i, 2] = new_bar\n",
    "\n",
    "        # Present the patch of bars to the truncated model.\n",
    "        y, _ = truncated_model(torch.tensor(bar_batch).type('torch.FloatTensor'),\n",
    "                               model, layer_idx)\n",
    "        yc, xc = calculate_center(y.shape[-2:])\n",
    "        center_responses[bar_i:bar_i+real_batch_size, :] = y[:, :, yc, xc].detach().numpy()\n",
    "        bar_i += real_batch_size\n",
    "\n",
    "    return center_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################.#######################################\n",
    "#                                                                             #\n",
    "#                                RFMP4a_RUN_01b                               #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "def rfmp4a_run_01b(model, model_name, result_dir, _debug=False):\n",
    "    \"\"\"\n",
    "    Map the RF of all conv layers in model using RF mapping paradigm 4a.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model      - neural network.\n",
    "    model_name - name of neural network. Used for txt file naming.\n",
    "    result_dir - directories to save the npy, txt, and pdf files.\n",
    "    _debug     - if true, run in debug mode.\n",
    "    \"\"\"\n",
    "    xn_list = xn_to_center_rf(model)  # Get the xn just big enough.\n",
    "    unit_counter = ConvUnitCounter(model)\n",
    "    layer_indices, nums_units = unit_counter.count()\n",
    "    _, max_rfs = get_rf_sizes(model, (227, 227), layer_type=nn.Conv2d)\n",
    "\n",
    "    delete_all_npy_files(result_dir)\n",
    "    delete_all_file_of_extension(result_dir, '.txt')\n",
    "    for conv_i in range(len(layer_indices)):\n",
    "        layer_name = f\"conv{conv_i + 1}\"\n",
    "        print(f\"{layer_name}\\n\")\n",
    "        # Get layer-specific info\n",
    "        xn = xn_list[conv_i]\n",
    "        layer_idx = layer_indices[conv_i]\n",
    "        num_units = nums_units[conv_i]\n",
    "        max_rf = max_rfs[conv_i][0]\n",
    "        splist = stimset_dict_rfmp_4a(xn, max_rf)\n",
    "        \n",
    "        # Array initializations\n",
    "        max_maps = np.zeros((num_units, max_rf, max_rf))\n",
    "        min_maps = np.zeros((num_units, max_rf, max_rf))\n",
    "        padding = (xn - max_rf)//2\n",
    "        \n",
    "        center_responses = barmap_run_01b(splist, model, layer_idx,\n",
    "                                          num_units, batch_size=100,\n",
    "                                          _debug=_debug)\n",
    "        \n",
    "        # Create txt files that summarize the top and bottom bars.\n",
    "        tb1_path = os.path.join(result_dir, f\"{model_name}_rfmp4a_tb1.txt\")\n",
    "        tb20_path = os.path.join(result_dir, f\"{model_name}_rfmp4a_tb20.txt\")\n",
    "        tb100_path = os.path.join(result_dir, f\"{model_name}_rfmp4a_tb100.txt\")\n",
    "        summarize_TB1(splist, center_responses, layer_name, tb1_path)\n",
    "        summarize_TBn(splist, center_responses, layer_name, tb20_path, top_n=20)\n",
    "        summarize_TBn(splist, center_responses, layer_name, tb100_path, top_n=100)\n",
    "\n",
    "        # Create maps of top/bottom bar average maps.\n",
    "        for unit_i in range(num_units):\n",
    "            if _debug and (unit_i > 10):\n",
    "                break\n",
    "            print_progress(f\"Making maps for unit {unit_i}...\")\n",
    "            max_map, min_map = mrfmap_make_non_overlap_map(splist, center_responses,\n",
    "                                unit_i, response_thr=0.1, stim_thr=0.2, _debug=_debug)\n",
    "            max_maps[unit_i] = max_map[padding:padding+max_rf, padding:padding+max_rf]\n",
    "            min_maps[unit_i] = min_map[padding:padding+max_rf, padding:padding+max_rf]\n",
    "        \n",
    "        # Save the maps of all units.\n",
    "        max_maps_path = os.path.join(result_dir, f\"{layer_name}_max_maps.npy\")\n",
    "        min_maps_path = os.path.join(result_dir, f\"{layer_name}_min_maps.npy\")\n",
    "        np.save(max_maps_path, max_maps)\n",
    "        np.save(min_maps_path, min_maps)\n",
    "\n",
    "        # Make pdf for the layer.\n",
    "        pdf_path = os.path.join(result_dir, f\"{layer_name}_maps.pdf\")\n",
    "        make_map_pdf(max_maps, min_maps, pdf_path, show=_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle(f\"conv{conv_i+1} unit no.{unit_i}\", fontsize=20)\n",
    "fig.set_size_inches(10, 5)\n",
    "fig.suptitle(f\"conv{conv_i+1} unit no.{unit_i}\", fontsize=20)\n",
    "ax1.clear()\n",
    "ax2.clear()\n",
    "im1 = ax1.imshow(np.zeros((227, 227)), cmap='gray', vmin=-1, vmax=1)\n",
    "im2 = ax2.imshow(np.zeros((227, 227)), cmap='gray')\n",
    "    \n",
    "def animate_func(frame):\n",
    "    im1.set_data(frame[3])\n",
    "    ax1.set_title(f\"response = {frame[1]:.2f}\")\n",
    "\n",
    "    vmin = im2.get_array().min()\n",
    "    vmax = im2.get_array().max()\n",
    "    im2.set_data(frame[0])\n",
    "    im2.set_clim(vmin=vmin, vmax=vmax)\n",
    "    ax2.set_title(f\"frame {frame[2]}\")\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, animate_func, frames=a, interval=10, save_count=0, cache_frame_data=False, repeat=False)\n",
    "\n",
    "ax1.add_patch(make_box(bm.box))\n",
    "ax2.add_patch(make_box(bm.box))\n",
    "boundary = bm.rf_size//2\n",
    "ax1.set_xlim([bm.box[1] - boundary, bm.box[3] + boundary])\n",
    "ax1.set_ylim([bm.box[0] - boundary, bm.box[2] + boundary])\n",
    "ax2.set_xlim([bm.box[1] - boundary, bm.box[3] + boundary])\n",
    "ax2.set_ylim([bm.box[0] - boundary, bm.box[2] + boundary])\n",
    "ax1.invert_yaxis()\n",
    "ax2.invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
